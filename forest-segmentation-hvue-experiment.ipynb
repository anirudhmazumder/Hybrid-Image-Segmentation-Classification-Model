{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b392afa1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-20T10:21:39.280525Z",
     "iopub.status.busy": "2025-11-20T10:21:39.280290Z",
     "iopub.status.idle": "2025-11-20T10:23:00.016344Z",
     "shell.execute_reply": "2025-11-20T10:23:00.015627Z"
    },
    "papermill": {
     "duration": 80.742205,
     "end_time": "2025-11-20T10:23:00.018061",
     "exception": false,
     "start_time": "2025-11-20T10:21:39.275856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae4dbd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:00.058898Z",
     "iopub.status.busy": "2025-11-20T10:23:00.058218Z",
     "iopub.status.idle": "2025-11-20T10:23:08.176549Z",
     "shell.execute_reply": "2025-11-20T10:23:08.175912Z"
    },
    "papermill": {
     "duration": 8.140051,
     "end_time": "2025-11-20T10:23:08.177987",
     "exception": false,
     "start_time": "2025-11-20T10:23:00.037936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import gc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd36ff66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:08.217747Z",
     "iopub.status.busy": "2025-11-20T10:23:08.217102Z",
     "iopub.status.idle": "2025-11-20T10:23:08.228118Z",
     "shell.execute_reply": "2025-11-20T10:23:08.227537Z"
    },
    "papermill": {
     "duration": 0.031935,
     "end_time": "2025-11-20T10:23:08.229313",
     "exception": false,
     "start_time": "2025-11-20T10:23:08.197378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SegmentationData(Dataset):    \n",
    "    def __init__(self, image_paths, mask_paths, metadata, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.metadata = metadata.reset_index(drop=True)  # Ensure clean index\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def get_image_path(self, image_id):\n",
    "        return f\"{self.image_paths}{image_id}\"\n",
    "\n",
    "    def get_mask_path(self, mask_id):\n",
    "        return f\"{self.mask_paths}{mask_id}\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get file paths\n",
    "        image_path = self.get_image_path(self.metadata.iloc[idx]['image'])\n",
    "        mask_path = self.get_mask_path(self.metadata.iloc[idx]['mask'])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Failed to load mask: {mask_path}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            \n",
    "            # Ensure mask has channel dimension (1, H, W) and is float\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "            mask = mask.float() / 255.0  # Normalize to [0, 1] and ensure float\n",
    "        else:\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        return image, mask\n",
    "            \n",
    "    def train_test_split(self, train_ratio=0.8):\n",
    "        total_samples = len(self.metadata)\n",
    "        train_size = int(total_samples * train_ratio)\n",
    "        \n",
    "        train_metadata = self.metadata.iloc[:train_size].reset_index(drop=True)\n",
    "        test_metadata = self.metadata.iloc[train_size:].reset_index(drop=True)\n",
    "\n",
    "        train_dataset = SegmentationData(\n",
    "            image_paths=self.image_paths,\n",
    "            mask_paths=self.mask_paths,\n",
    "            metadata=train_metadata,\n",
    "            transforms=self.transforms\n",
    "        )\n",
    "\n",
    "        test_dataset = SegmentationData(\n",
    "            image_paths=self.image_paths,\n",
    "            mask_paths=self.mask_paths,\n",
    "            metadata=test_metadata,\n",
    "            transforms=self.transforms\n",
    "        )\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def training_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=45,\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(p=0.3),\n",
    "            A.GridDistortion(p=0.3),\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "        ], p=0.3),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def validation_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c051f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:08.268275Z",
     "iopub.status.busy": "2025-11-20T10:23:08.268027Z",
     "iopub.status.idle": "2025-11-20T10:23:19.697189Z",
     "shell.execute_reply": "2025-11-20T10:23:19.696349Z"
    },
    "papermill": {
     "duration": 11.450617,
     "end_time": "2025-11-20T10:23:19.698597",
     "exception": false,
     "start_time": "2025-11-20T10:23:08.247980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = ConvolutionalBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.conv_block(x)\n",
    "        pooled = self.pool(skip)\n",
    "        return skip, pooled\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_block = ConvolutionalBlock(out_channels + skip_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n",
    "        \n",
    "        # Extract features from layer2 (good mid-level features)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,  # 256 channels\n",
    "            resnet.layer2,  # 512 channels\n",
    "        )\n",
    "        self.out_channels = 512\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((16, 16))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = self.adaptive_pool(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class VisionTransformerFeatures(nn.Module):\n",
    "    def __init__(self, img_size=256, output_channels=3):\n",
    "        super(VisionTransformerFeatures, self).__init__()\n",
    "        \n",
    "        self.vit = timm.create_model('vit_tiny_patch16_224', pretrained=False, img_size=img_size)\n",
    "        vit_features = self.vit.embed_dim\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(vit_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_channels * 16 * 16)\n",
    "        )\n",
    "        self.output_channels = output_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        \n",
    "        vit_out = self.vit.forward_features(x)\n",
    "        vit_out = vit_out.mean(dim=1)\n",
    "        \n",
    "        vit_out = self.projection(vit_out)\n",
    "        vit_out = vit_out.view(B, self.output_channels, 16, 16)\n",
    "        \n",
    "        return vit_out\n",
    "\n",
    "\n",
    "class HVUEArchitecture(nn.Module):\n",
    "    def __init__(self, num_classes=4, img_size=256, pretrained=True):\n",
    "        super(HVUEArchitecture, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # ResNet feature extractor (upgraded from DenseNet)\n",
    "        self.resnet_features = ResNetFeatureExtractor(pretrained=pretrained)\n",
    "        resnet_channels = self.resnet_features.out_channels\n",
    "        \n",
    "        # Vision Transformer\n",
    "        self.vit = VisionTransformerFeatures(img_size=img_size, output_channels=3)\n",
    "        vit_channels = self.vit.output_channels\n",
    "        \n",
    "        # U-Net Encoder (3 channels for RGB images)\n",
    "        self.enc1 = EncoderBlock(3, 64)  # Changed from 2 to 3 for RGB\n",
    "        self.enc2 = EncoderBlock(64, 128)\n",
    "        self.enc3 = EncoderBlock(128, 256)\n",
    "        self.enc4 = EncoderBlock(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvolutionalBlock(512, 1024)\n",
    "        \n",
    "        # Feature fusion at bottleneck\n",
    "        fusion_channels = 1024 + resnet_channels + vit_channels\n",
    "        \n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(fusion_channels, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # U-Net Decoder\n",
    "        self.dec1 = DecoderBlock(1024, 512, 512)\n",
    "        self.dec2 = DecoderBlock(512, 256, 256)\n",
    "        self.dec3 = DecoderBlock(256, 128, 128)\n",
    "        self.dec4 = DecoderBlock(128, 64, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # U-Net Encoder with skip connections\n",
    "        s1, p1 = self.enc1(x)\n",
    "        s2, p2 = self.enc2(p1)\n",
    "        s3, p3 = self.enc3(p2)\n",
    "        s4, p4 = self.enc4(p3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)\n",
    "        \n",
    "        # ResNet features (upgraded from DenseNet)\n",
    "        resnet_feat = self.resnet_features(x)\n",
    "        \n",
    "        # Vision Transformer features\n",
    "        vit_feat = self.vit(x)\n",
    "        \n",
    "        # Concatenate all features at bottleneck\n",
    "        fused = torch.cat([b, resnet_feat, vit_feat], dim=1)\n",
    "        \n",
    "        # Reduce channels through fusion\n",
    "        fused = self.fusion_conv(fused)\n",
    "        \n",
    "        # U-Net Decoder with skip connections\n",
    "        d1 = self.dec1(fused, s4)\n",
    "        d2 = self.dec2(d1, s3)\n",
    "        d3 = self.dec3(d2, s2)\n",
    "        d4 = self.dec4(d3, s1)\n",
    "        \n",
    "        # Output (logits)\n",
    "        out = self.final_conv(d4)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3bfede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:19.738192Z",
     "iopub.status.busy": "2025-11-20T10:23:19.737932Z",
     "iopub.status.idle": "2025-11-20T10:23:19.767844Z",
     "shell.execute_reply": "2025-11-20T10:23:19.767261Z"
    },
    "papermill": {
     "duration": 0.051078,
     "end_time": "2025-11-20T10:23:19.768916",
     "exception": false,
     "start_time": "2025-11-20T10:23:19.717838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "def dice_coef_binary(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = y_true.contiguous().view(-1)\n",
    "    y_pred_f = y_pred.contiguous().view(-1)\n",
    "    \n",
    "    intersection = torch.sum(y_true_f * y_pred_f)\n",
    "    dice = (2.0 * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "\n",
    "def train(train_dataset, val_dataset, model, epochs=50, batch_size=2, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        num_workers = 2\n",
    "        pin_memory = True\n",
    "        prefetch_factor = 2\n",
    "    else:\n",
    "        num_workers = 0\n",
    "        pin_memory = False\n",
    "        prefetch_factor = None\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=prefetch_factor\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=prefetch_factor\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5, min_lr=1e-7, verbose=True\n",
    "    )\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Mixed Precision (AMP): {use_amp}\")\n",
    "    print(f\"  Num Workers: {num_workers}\")\n",
    "    print(f\"  Batch Size: {batch_size} (REDUCED for memory safety)\")\n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"  Total epochs: {epochs}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            \n",
    "            # Ensure masks have correct shape (B, 1, H, W) and are float\n",
    "            if masks.dim() == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "            masks = masks.float()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    \n",
    "                    # Compute dice for monitoring\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    dice = dice_coef_binary(masks, probs)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping to prevent explosion\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                probs = torch.sigmoid(outputs)\n",
    "                dice = dice_coef_binary(masks, probs)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping to prevent explosion\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "            \n",
    "            # Check for NaN\n",
    "            if torch.isnan(loss) or torch.isnan(dice):\n",
    "                print(f\"\\n⚠ WARNING: NaN detected at batch {batch_idx}. Skipping batch...\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_dice += dice.item() * images.size(0)\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'dice': f'{dice.item():.4f}'\n",
    "            })\n",
    "            \n",
    "            if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataset)\n",
    "        avg_train_dice = train_dice / len(train_dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_dice = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "            for images, masks in val_pbar:\n",
    "                try:\n",
    "                    images = images.to(device, non_blocking=True)\n",
    "                    masks = masks.to(device, non_blocking=True)\n",
    "                    \n",
    "                    # Ensure masks have correct shape (B, 1, H, W) and are float\n",
    "                    if masks.dim() == 3:\n",
    "                        masks = masks.unsqueeze(1)\n",
    "                    masks = masks.float()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(images)\n",
    "                            loss = criterion(outputs, masks)\n",
    "                            probs = torch.sigmoid(outputs)\n",
    "                            dice = dice_coef_binary(masks, probs)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, masks)\n",
    "                        probs = torch.sigmoid(outputs)\n",
    "                        dice = dice_coef_binary(masks, probs)\n",
    "                    \n",
    "                    # Accumulate metrics\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    val_dice += dice.item() * images.size(0)\n",
    "                    \n",
    "                    val_pbar.set_postfix({\n",
    "                        'loss': f'{loss.item():.4f}',\n",
    "                        'dice': f'{dice.item():.4f}'\n",
    "                    })\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        print(f\"\\n⚠ WARNING: OOM during validation. Skipping batch...\")\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise e\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataset)\n",
    "        avg_val_dice = val_dice / len(val_dataset)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Dice: {avg_train_dice:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Dice: {avg_val_dice:.4f}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB / {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB (peak)\")\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_dice': avg_val_dice,\n",
    "            }, 'best_model_hvue.pth')\n",
    "            print(f\"  ✓ Best model saved (Val Loss: {avg_val_loss:.4f})\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n✓ Training completed!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataset, batch_size=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        num_workers = 2\n",
    "        pin_memory = True\n",
    "    else:\n",
    "        num_workers = 0\n",
    "        pin_memory = False\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_dice = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_pixel_acc = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_f1 = 0.0\n",
    "    total_specificity = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # For AUC-ROC calculation\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating on {len(test_dataset)} samples...\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc='Evaluating'):\n",
    "            try:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "                \n",
    "                # Ensure masks have correct shape (B, 1, H, W) and are float\n",
    "                if masks.dim() == 3:\n",
    "                    masks = masks.unsqueeze(1)\n",
    "                masks = masks.float()\n",
    "                \n",
    "                # Forward pass\n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        probs = torch.sigmoid(outputs)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                \n",
    "                # Binary predictions (threshold at 0.5)\n",
    "                preds = (probs > 0.5).float()\n",
    "                \n",
    "                # Flatten for metric calculation\n",
    "                masks_flat = masks.view(-1)\n",
    "                preds_flat = preds.view(-1)\n",
    "                probs_flat = probs.view(-1)\n",
    "                \n",
    "                # Compute metrics\n",
    "                # 1. Dice Coefficient (using binary predictions for consistency)\n",
    "                dice = dice_coef_binary(masks, preds)  # Changed from probs to preds\n",
    "                \n",
    "                # 2. IoU (Intersection over Union / Jaccard Index)\n",
    "                intersection = torch.sum(masks_flat * preds_flat)\n",
    "                union = torch.sum(masks_flat) + torch.sum(preds_flat) - intersection\n",
    "                iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "                \n",
    "                # 3. Pixel Accuracy\n",
    "                pixel_acc = torch.sum(preds_flat == masks_flat) / masks_flat.numel()\n",
    "                \n",
    "                # 4. Precision, Recall, F1\n",
    "                true_positives = torch.sum(masks_flat * preds_flat)\n",
    "                false_positives = torch.sum((1 - masks_flat) * preds_flat)\n",
    "                false_negatives = torch.sum(masks_flat * (1 - preds_flat))\n",
    "                true_negatives = torch.sum((1 - masks_flat) * (1 - preds_flat))\n",
    "                \n",
    "                precision = (true_positives + 1e-6) / (true_positives + false_positives + 1e-6)\n",
    "                recall = (true_positives + 1e-6) / (true_positives + false_negatives + 1e-6)\n",
    "                f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "                \n",
    "                # 5. Specificity\n",
    "                specificity = (true_negatives + 1e-6) / (true_negatives + false_positives + 1e-6)\n",
    "                \n",
    "                # Skip if NaN\n",
    "                if torch.isnan(dice) or torch.isnan(iou):\n",
    "                    print(f\"\\n⚠ WARNING: NaN detected in evaluation. Skipping batch...\")\n",
    "                    continue\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                total_dice += dice.item() * images.size(0)\n",
    "                total_iou += iou.item() * images.size(0)\n",
    "                total_pixel_acc += pixel_acc.item() * images.size(0)\n",
    "                total_precision += precision.item() * images.size(0)\n",
    "                total_recall += recall.item() * images.size(0)\n",
    "                total_f1 += f1.item() * images.size(0)\n",
    "                total_specificity += specificity.item() * images.size(0)\n",
    "                total_samples += images.size(0)\n",
    "                \n",
    "                # Collect for AUC-ROC (sample to avoid memory issues)\n",
    "                if len(all_probs) < 10000:  # Limit samples for AUC calculation\n",
    "                    all_probs.append(probs_flat.cpu())\n",
    "                    all_labels.append(masks_flat.cpu())\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"\\n⚠ WARNING: OOM during evaluation. Skipping batch...\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_dice = total_dice / total_samples if total_samples > 0 else 0\n",
    "    avg_iou = total_iou / total_samples if total_samples > 0 else 0\n",
    "    avg_pixel_acc = total_pixel_acc / total_samples if total_samples > 0 else 0\n",
    "    avg_precision = total_precision / total_samples if total_samples > 0 else 0\n",
    "    avg_recall = total_recall / total_samples if total_samples > 0 else 0\n",
    "    avg_f1 = total_f1 / total_samples if total_samples > 0 else 0\n",
    "    avg_specificity = total_specificity / total_samples if total_samples > 0 else 0\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPREHENSIVE EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n Primary Metrics:\")\n",
    "    print(f\"  Dice Coefficient:        {avg_dice:.4f}  (Overlap-based metric)\")\n",
    "    print(f\"  IoU (Jaccard Index):     {avg_iou:.4f}  (Intersection over Union)\")\n",
    "    print(f\"  Pixel Accuracy:          {avg_pixel_acc:.4f}  (Correct pixels / Total pixels)\")\n",
    "    \n",
    "    print(f\"\\n Classification Metrics:\")\n",
    "    print(f\"  Precision:               {avg_precision:.4f}  (TP / (TP + FP))\")\n",
    "    print(f\"  Recall (Sensitivity):    {avg_recall:.4f}  (TP / (TP + FN))\")\n",
    "    print(f\"  F1 Score:                {avg_f1:.4f}  (Harmonic mean of Precision & Recall)\")\n",
    "    print(f\"  Specificity:             {avg_specificity:.4f}  (TN / (TN + FP))\")\n",
    "        \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'dice': avg_dice,\n",
    "        'iou': avg_iou,\n",
    "        'pixel_accuracy': avg_pixel_acc,\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1_score': avg_f1,\n",
    "        'specificity': avg_specificity,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcea3e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:19.809058Z",
     "iopub.status.busy": "2025-11-20T10:23:19.808568Z",
     "iopub.status.idle": "2025-11-20T10:23:21.912219Z",
     "shell.execute_reply": "2025-11-20T10:23:21.911385Z"
    },
    "papermill": {
     "duration": 2.125298,
     "end_time": "2025-11-20T10:23:21.913401",
     "exception": false,
     "start_time": "2025-11-20T10:23:19.788103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading metadata...\n",
      "    Total samples: 5108\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "base_path = \"/kaggle/input/augmented-forest-segmentation/Forest Segmented/Forest Segmented/\"\n",
    "image_paths = base_path + 'images/'\n",
    "mask_paths = base_path + 'masks/'\n",
    "metadata_path = base_path + 'meta_data.csv'\n",
    "\n",
    "print(\"\\n[1] Loading metadata...\")\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "print(f\"    Total samples: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210eb233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:21.952763Z",
     "iopub.status.busy": "2025-11-20T10:23:21.952559Z",
     "iopub.status.idle": "2025-11-20T10:23:21.957158Z",
     "shell.execute_reply": "2025-11-20T10:23:21.956404Z"
    },
    "papermill": {
     "duration": 0.025487,
     "end_time": "2025-11-20T10:23:21.958178",
     "exception": false,
     "start_time": "2025-11-20T10:23:21.932691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] Creating dataset (on-demand loading, NO preloading)...\n",
      "    ✓ Dataset created (images will load on-the-fly)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2] Creating dataset (on-demand loading, NO preloading)...\")\n",
    "dataset = SegmentationData(\n",
    "    image_paths=image_paths,\n",
    "    mask_paths=mask_paths,\n",
    "    metadata=metadata,\n",
    "    transforms=None\n",
    ")\n",
    "print(\"    ✓ Dataset created (images will load on-the-fly)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0effc7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:21.997688Z",
     "iopub.status.busy": "2025-11-20T10:23:21.997110Z",
     "iopub.status.idle": "2025-11-20T10:23:22.009644Z",
     "shell.execute_reply": "2025-11-20T10:23:22.008763Z"
    },
    "papermill": {
     "duration": 0.033471,
     "end_time": "2025-11-20T10:23:22.010723",
     "exception": false,
     "start_time": "2025-11-20T10:23:21.977252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] Splitting dataset...\n",
      "    Train samples: 3575\n",
      "    Test samples: 1533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3] Splitting dataset...\")\n",
    "train_dataset, test_dataset = dataset.train_test_split(train_ratio=0.7)\n",
    "\n",
    "train_dataset.transforms = training_transforms()\n",
    "test_dataset.transforms = validation_transforms()\n",
    "\n",
    "print(f\"    Train samples: {len(train_dataset)}\")\n",
    "print(f\"    Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01733688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:22.050012Z",
     "iopub.status.busy": "2025-11-20T10:23:22.049768Z",
     "iopub.status.idle": "2025-11-20T10:23:23.779320Z",
     "shell.execute_reply": "2025-11-20T10:23:23.778327Z"
    },
    "papermill": {
     "duration": 1.750755,
     "end_time": "2025-11-20T10:23:23.780627",
     "exception": false,
     "start_time": "2025-11-20T10:23:22.029872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] Creating model with ResNet50 backbone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 187MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Using ResNet50 (upgraded from DenseNet)\n",
      "    ✓ Pretrained on ImageNet: Yes\n",
      "    Total parameters: 62,090,921\n",
      "    Trainable parameters: 62,090,921\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4] Creating model with ResNet50 backbone...\")\n",
    "model = HVUEArchitecture(\n",
    "    num_classes=1,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"    ✓ Using ResNet50 (upgraded from DenseNet)\")\n",
    "print(f\"    ✓ Pretrained on ImageNet: Yes\")\n",
    "print(f\"    Total parameters: {total_params:,}\")\n",
    "print(f\"    Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed86a5a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:23:23.822098Z",
     "iopub.status.busy": "2025-11-20T10:23:23.821858Z",
     "iopub.status.idle": "2025-11-20T10:42:39.667263Z",
     "shell.execute_reply": "2025-11-20T10:42:39.666565Z"
    },
    "papermill": {
     "duration": 1155.867095,
     "end_time": "2025-11-20T10:42:39.668606",
     "exception": false,
     "start_time": "2025-11-20T10:23:23.801511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5] Training model with MEMORY-SAFE settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Configuration:\n",
      "================================================================================\n",
      "  Device: cuda\n",
      "  Mixed Precision (AMP): True\n",
      "  Num Workers: 2\n",
      "  Batch Size: 2 (REDUCED for memory safety)\n",
      "  Train samples: 3575\n",
      "  Validation samples: 1533\n",
      "  Total epochs: 5\n",
      "  GPU Memory Available: 15.83 GB\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 1788/1788 [03:28<00:00,  8.58it/s, loss=0.8789, dice=0.7177]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 767/767 [00:25<00:00, 30.04it/s, loss=0.4296, dice=0.1425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 1/5 Summary:\n",
      "================================================================================\n",
      "  Train Loss: 0.5215 | Train Dice: 0.6741\n",
      "  Val Loss: 0.4502 | Val Dice: 0.7017\n",
      "  GPU Memory: 1.02 GB / 14.91 GB (peak)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Best model saved (Val Loss: 0.4502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 1788/1788 [03:24<00:00,  8.73it/s, loss=0.5261, dice=0.8362]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 767/767 [00:24<00:00, 31.67it/s, loss=0.3245, dice=0.1606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 2/5 Summary:\n",
      "================================================================================\n",
      "  Train Loss: 0.4651 | Train Dice: 0.7142\n",
      "  Val Loss: 0.4715 | Val Dice: 0.6845\n",
      "  GPU Memory: 1.02 GB / 1.44 GB (peak)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 1788/1788 [03:25<00:00,  8.70it/s, loss=0.2558, dice=0.8524]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 767/767 [00:24<00:00, 31.76it/s, loss=0.6687, dice=0.1031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 3/5 Summary:\n",
      "================================================================================\n",
      "  Train Loss: 0.4400 | Train Dice: 0.7326\n",
      "  Val Loss: 0.4159 | Val Dice: 0.7297\n",
      "  GPU Memory: 1.02 GB / 1.44 GB (peak)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Best model saved (Val Loss: 0.4159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 1788/1788 [03:25<00:00,  8.70it/s, loss=0.3273, dice=0.7736]\n",
      "Epoch 4/5 [Val]: 100%|██████████| 767/767 [00:23<00:00, 31.96it/s, loss=0.4045, dice=0.1333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 4/5 Summary:\n",
      "================================================================================\n",
      "  Train Loss: 0.4277 | Train Dice: 0.7343\n",
      "  Val Loss: 0.4249 | Val Dice: 0.7059\n",
      "  GPU Memory: 1.02 GB / 1.44 GB (peak)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 1788/1788 [03:25<00:00,  8.72it/s, loss=0.2482, dice=0.8767]\n",
      "Epoch 5/5 [Val]: 100%|██████████| 767/767 [00:23<00:00, 32.16it/s, loss=0.5114, dice=0.1228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 5/5 Summary:\n",
      "================================================================================\n",
      "  Train Loss: 0.4256 | Train Dice: 0.7424\n",
      "  Val Loss: 0.4350 | Val Dice: 0.6914\n",
      "  GPU Memory: 1.02 GB / 1.44 GB (peak)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✓ Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5] Training model with MEMORY-SAFE settings...\")\n",
    "\n",
    "trained_model = train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=test_dataset,\n",
    "    model=model,\n",
    "    epochs=5,\n",
    "    batch_size=2,\n",
    "    learning_rate=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87be4596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:42:41.698953Z",
     "iopub.status.busy": "2025-11-20T10:42:41.698157Z",
     "iopub.status.idle": "2025-11-20T10:43:06.384549Z",
     "shell.execute_reply": "2025-11-20T10:43:06.383516Z"
    },
    "papermill": {
     "duration": 25.703357,
     "end_time": "2025-11-20T10:43:06.385891",
     "exception": false,
     "start_time": "2025-11-20T10:42:40.682534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6] Evaluating model...\n",
      "\n",
      "================================================================================\n",
      "Evaluating on 1533 samples...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 767/767 [00:24<00:00, 31.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      " Primary Metrics:\n",
      "  Dice Coefficient:        0.7650  (Overlap-based metric)\n",
      "  IoU (Jaccard Index):     0.6750  (Intersection over Union)\n",
      "  Pixel Accuracy:          0.8015  (Correct pixels / Total pixels)\n",
      "\n",
      " Classification Metrics:\n",
      "  Precision:               0.8547  (TP / (TP + FP))\n",
      "  Recall (Sensitivity):    0.7663  (TP / (TP + FN))\n",
      "  F1 Score:                0.7650  (Harmonic mean of Precision & Recall)\n",
      "  Specificity:             0.7241  (TN / (TN + FP))\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6] Evaluating model...\")\n",
    "\n",
    "results = evaluate(\n",
    "    model=trained_model,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd666dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:43:08.327578Z",
     "iopub.status.busy": "2025-11-20T10:43:08.326717Z",
     "iopub.status.idle": "2025-11-20T10:43:08.989446Z",
     "shell.execute_reply": "2025-11-20T10:43:08.988538Z"
    },
    "papermill": {
     "duration": 1.685573,
     "end_time": "2025-11-20T10:43:08.990563",
     "exception": false,
     "start_time": "2025-11-20T10:43:07.304990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7] Saving final model...\n",
      "    ✓ Model saved to trained_model_final.pth\n",
      "    ✓ Memory cleaned up\n",
      "\n",
      "================================================================================\n",
      "✓ TRAINING COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[7] Saving final model...\")\n",
    "torch.save(trained_model.state_dict(), 'trained_model_final.pth')\n",
    "print(\"    ✓ Model saved to trained_model_final.pth\")\n",
    "\n",
    "# Final cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"    ✓ Memory cleaned up\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1650618,
     "sourceId": 2738848,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1298.477852,
   "end_time": "2025-11-20T10:43:12.621450",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T10:21:34.143598",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
